---
title: "Homework for Chapter 16: Fixed effects"
author: "Braulio Guemez"
date: "3/2/2022"
output: html_document
---

## Homework 8

### 1. You observe the number of vacations taken by Zac and Skylar in 2012, 2013, and 2014. In those years, Zac took 3, 7, and 5 vacations, respectively. Skylar took 2, 6, and 10.

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(ggthemes)
library(modelsummary)
library(tidyr)
library(broom)
library(car)
library(fixest)
```

```{r}
 d <- tibble(year=c(2012,2013,2014,2012,2013,2014), vac=c(3,7,5,2,6,10), person=c(1,1,1,0,0,0) ) # 1 is zac , 0 skylar 


d
```

#### a. Isolate the numbers that represent the variation between Zac and Skylar in their vacation-taking.

```{r}

d |> group_by(person) |> summarize(mean_Vac=mean(vac)) 
```

*The between variation is the difference between the mean of Zac (6) and the mean of Skylar (5), which is 1.*

#### b. Isolate the variation within Zac and within Skylar in their vacation-taking.

```{r}
 d |> mutate(ind_mean=c(rep(6,3), rep(5,3)),
             within_var=vac-ind_mean) 
```

*The within-individual variation is the difference between the average number of vacations for each individual minus each individual's number of vacations for a given year*

#### c. (Difficult!) We perform a fixed effects analysis of the effect of vacations on happiness. A vacation increases Zac's happiness by 1 "happiness point," but it increases Skylar's happiness by 2 "happiness points." Will our fixed effects estimate likely give us an answer closer to 1, closer to 2, or exactly 1.5?

*I suspect the slopes will average out and the the result will be exactly 1.5. But let's check that out*

```{r}

c <- tibble(vacation=c(rep(1:5,2)), hap=c(1:5,2,4,6,8,10), per=c(rep(1,5),
                                                        rep(0,5)    )) 
c |> ggplot(aes(vacation,hap,col=per, group=per)) + geom_point() + geom_smooth(method="lm") + theme_clean()

m1 <-  feols(hap ~ vacation | per, data = c)
tidy(m1)

c <- c |> mutate(predict=c(1:5*1.5+rep(-1.5,5), 1:5*1.5+rep(1.5,5)))  ## this looks like a mess. I am just using the values of the eq to estimate the predictions

c |> ggplot(aes(vacation,hap)) + geom_point(aes(shape=as.character(per))) + theme_clean() + geom_line(aes(vacation, predict, col=per, group=per)) + guides(shape="none", group="none")

```


 



*The analysis confirms that the answer is 1.5 as indicated by the fixed effects model*


### 2. You are interested in the effect of cultural events on the levels of trust in a city. Perhaps big events like concerts bring people together and they can trust each other more. You plan to look at the relationship between trust and number of events in a given year, with fixed effects for city. Draw a causal diagram for this research question with at least four back door paths. Which paths will be closed by fixed effects, and which will remain open?


```{r}
knitr::include_graphics("C:/Users/braul/OneDrive - Duke University/1. First Year/3. Stats/soc723/8. hw_8/pic1.png")
```

Fixed effects of cities will  control for city, infrastructure and public funding in a year, because those are variables nested in "city" that are stable --assuming public funding is allocated once a year. Police violence (derived from general violence) is a variable that may change within a year in a city, so this back door will remain open.




### 3. Classify each of the following forms of variation as "between variation", "within variation", or a combination of both.

#### a.  (Individual = person) How a child's height changes as they age.

*Within variation*

#### b.  (Individual = person) In a data set tracking many people over many years, the variation in the number of children a person has in a given year.

*Within variation*

#### c.  (Individual = city) Overall, Paris, France has more restaurants than Paris, Texas.

*Between variation*

#### d.  (Individual = genre) The average pop music album sells more copies than the average jazz album

*Between variation*

#### e.  (Individual = genre) Miles Davis' Kind of Blue sold very well for a jazz album.

*Combination of both. It can be between variation if you compare Kind of Blue with other albums in others genres. It can also be within variation if you compare Miles David with other jazz albums.*


#### f.  (Individual = genre) Michael Jackson's Thriller, a pop album, sold many more copies than Kind of Blue, a jazz album.

*Between variation*


### 4.  Why does the process of taking each observation relative to its individual-level mean have the effect of "controlling for individual"? How is it Performed?

*Because individuals are compared against themselves over time and their individual-level characteristics remain stable. Changes at the individual-level cannot be explained by those characteristics. This is like a within-person experiment, where the individual is given a treatment multiple times. Individual characteristics like gender, height, race, socioeconomic origin, are "controlled for" because they remain constant*



### 5.  You are interested in the effect of cultural events on the levels of trust in a city. You run a regression of trust levels (on a 0-100 scale) on the number of cultural events with city fixed effects and get a coefficient on cultural events of 3.6. Assume that there are still some back doors open, so do not interpret the result causally. Interpret the 3.6, explaining it in an English sentence.

*For a given city, every additional cultural event is linearly _associated_ to an increase of 3.6 points on trust levels.* 


### 6.  You are interested in the effect of cultural events on the levels of trust in a city. You run a regression of trust levels (on a 0-100 scale) on the number of cultural events with city and year fixed effects and get a coefficient on cultural events of 2.4. Assume that there are still some back doors open, so do not interpret the result causally. Interpret the 2.4, explaining it in an English sentence.

*For a given city and for a given year, every additional cultural event is linearly _associated_ to an increase of 2.4 points on trust levels.* 


### 7.  Two-way fixed effects with terms for both individual and time are often referred to as "controlling for individual and time effects". Why might a researcher want to do this rather than just taking individual fixed effects and adding a linear/polynomial/etc. term for time?

*Adding terms for years does not control for unobserved variables nested within years, and as a consequence it won't close all the possible back-doors.*


### 8.  Which of the following explains why random effects is likely to do a better job of estimating the individual-level effects than fixed effects, if its assumptions hold?

*c.	Because it uses the information from the entire data set to estimate each individual effect, rather than relying on only a few observations per individual.*


## Coding Homework

### 1. Load the `mathpnl.csv` data file provided (in R or Python store it as `mp`), which comes from Leslie Papke and consists of data at the school district level, and was featured in the Wooldridge (2010) textbook. 
   
   We are only going to be working with a few variables. Limit the data to these variables:
   
   - distid: the district identifier (our "individual" for fixed effects)
   - year: the year the data is from
   - math4: the percentage of 4th grade students who are "satisfactory" or better in math
   - expp: expenditure per pupil
   - lunch: the percentage of students eligible for free lunch
   
   
```{r}
mp <- read.csv("C:/Users/braul/OneDrive - Duke University/1. First Year/3. Stats/HW docs/mathpnl.csv")

mp <- mp |> select(distid,year,math4,expp,lunch)

glimpse(mp)

```
   
   
### 2. Panel data is often described as "N by T". That is, the number of different individuals N and the number of time periods T. Write code that outputs what N and T are in this data.

```{r}
length(unique(mp$distid)) 
length(unique(mp$year)) 
```


We have 550 individuals (districts) and 7 years.



### 3. A *balanced* panel is one in which each individual shows up in every single time period. You can check whether a data set is a balanced panel by seeing whether the number of unique time periods each individual ID shows up in is the same as the number of unique time periods, or whether the number of unique individual IDs in each time period is the same as the total number of unique individual IDs. Think to yourself a second about why these procedures would check that this is a balanced panel. Then, check whether this data set is a balanced panel.


*These procedures will tell us if we have enough variation to do our analysis. Missing data can introduce bias, so that is why we want to make sure our panel data is balanced.*

```{r}
mp_bal <- mp |> select(year, distid) |> mutate(uno=1) |> group_by(distid) |> summarize(num_years=n())
 
mp_bal |> ggplot(aes(distid, num_years)) + geom_point() + theme_clean()

```

*The data is balanced. We have information about all the individuals for the 7 available years.*


### 4. Run an OLS regression, with no fixed effects, of `math4` on `expp` and `lunch`. Store the results as `m1`.

```{r}
m1 <- lm(math4 ~ expp + lunch, data=mp)
```


### 5. Modify the model in step 4 to include fixed effects for `distid` "by hand". That is, subtract out the within-`distid` mean of `math4`, `expp`, and `lunch`, creating new variables `math4_demean`, `expp_demean`, and `lunch_demean`, and re-estimate the model using those variables, storing the result as `m2`. 

(tip: be careful that your code doesn't overwrite the original variables, or at least if it does, reload the data afterwards)

```{r}
mp_within <- mp |> group_by(distid) |> mutate(math4_demean = math4 - mean(math4),
                                              expp_demean = expp - mean(expp),
                                              lunch_demean = lunch - mean(lunch))

m2 <- lm(math4_demean ~ expp_demean + lunch_demean, data = mp_within)

```



### 6. Next we're going to estimate fixed effects by including `distid` as a set of dummies. This can be extremely slow, so for demonstration purposes use only the first 500 observations of your data (don't get rid of the other observations, though, you'll want them for the rest of this assignment). Run the model from step 4 but with dummies for different values of `distid`, saving the result as `m3`. Then, do a joint F test on the dummies (see Chapter 13), and report if you can reject that the dummies are jointly zero at the 99% level.

```{r}

reduced_mp <- head(mp, 500)

m3 <- lm(math4 ~ expp + lunch + factor(distid) -1, data=reduced_mp) 

char_id <- as.character(unique(reduced_mp$distid))

linearHypothesis(m3,gsub(" ", "", paste("factor(distid)",char_id))) 

```



*The output of the joint F test shows that including the intercept in the model significantly increases the predictive power of the model at the 99% level.*


### 7. Now we will use a specially-designed function to estimate a model with fixed effects. (Using the whole data set once again), use `feols()` from the **fixest** package in R to estimate the model from step 4 but with fixed effects for `distid`. Save the result as `m4`. Include standard errors clustered at the `distid` level.

```{r}

m4 <- feols(math4 ~ expp + lunch | distid, data = mp) # Note that standard errors will be clustered by the first 
# fixed effect 
```


### 8. Now add fixed effects for year to your model from step 7 to create a two-way fixed effects model. Keep the standard errors clustered at the `distid` level. Save the results as `m5`.


```{r}

m5 <- feols(math4 ~ expp + lunch | distid + year, data = mp) # Note that standard errors will be clustered by the first 
# fixed effect 
```


### 9. Using `modelsummary()` from **modelsummary** in R make a regression table including `m1` through `m5` in the same table so you can compare them all. Read the documentation of your command to figure out how to include the `expp`, `lunch`, `expp_demean`, and `lunch_demean` predictors in the table without clogging the thing up with a bunch of dummy coefficients from `m3`.

Write down two interesting things you notice from the table. Multiple possible answers here.


```{r}

  modelsummary(models = list(m1,m2,m3,m4,m5),
            coef_map = c('expp_demean',
                         'lunch_demean',
                         'expp',
                          'lunch'),
            gof_omit=".*") 


```


*1. Lunch has a negative effect on the first model. If we didn't know about fixed effects and were kind of naive, we would think that giving free lunch is actually negative for improving math scores.*

*2. When we adjust for district and years (versus only district) we see that both lunch and expenditure coefficients diminish. I suspect this is because there are additional intercepts for each year, which eat some of the variation from the first model.*


### 10. Finally, we'll close it out by using correlated random effects instead of fixed effects (see 16.3.3). You already have `expp_demean` and `lunch_demean` from earlier. Now, modify the code from that slightly to add on `expp_mean` and `lunch_mean` (the mean within `distid` instead of the value *minus* that mean). Then, regress `math4` on `expp_demean`, `lunch_demean`, `expp_mean`, and `lunch_mean`, with random effects for `distid` using `lmer()` from **lme4** in R.


```{r}

library(lme4)

mp$distid <- as.character(mp$distid)

db10 <- mp_within |> mutate(expp_mean = mean(expp),
              lunch_mean = mean(lunch)) 

m6 <- lmer(math4 ~ expp_demean + lunch_demean + expp_mean + lunch_mean | distid, data=db10)

modelsummary(m6)

```
With this model we estimated the within and between variation 
